{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter             # sys.version = 3.9.6\n",
    "import datetime as dt\n",
    "from IPython.display import clear_output    # 8.18.1\n",
    "import matplotlib.pyplot as plt             # 3.8.3\n",
    "import numpy as np                          # 1.26.4\n",
    "import random\n",
    "import seaborn as sns                       # 0.13.2\n",
    "import simpleaudio as sa                    # 1.0.4                                      \n",
    "import tensorflow as tf                     # 2.16.1\n",
    "from tensorflow import keras                # 2.16.1\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "seed = 111\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions independent of the model\n",
    "\n",
    "def start_round():\n",
    "    # Shuffle dominos and divde them between undrawn (U), the player (P), opponent (O), table (T)\n",
    "    dominos = [[i, j] for i in range(7) for j in range(i+1)]\n",
    "    random.shuffle(dominos)\n",
    "    U = dominos; P = []; O = []; T = []\n",
    "    for _ in range(7):\n",
    "        P.append(U.pop(0))\n",
    "    for _ in range(7):\n",
    "        O.append(U.pop(0))\n",
    "    return U, P, O, T\n",
    "\n",
    "def print_board(U, O, T, P, P_points, O_points):\n",
    "    print(f'Player points: {P_points} - Opponent points: {O_points}')\n",
    "    print(f'U: {U}')\n",
    "    print(f'O: {O}')\n",
    "    print(f'T: {T}')\n",
    "    print(f'P: {P}')\n",
    "\n",
    "def movable_dominos(X, T):\n",
    "    # Find which dominos can be placed on the left and right end of the chain\n",
    "    if len(T) == 0:\n",
    "        movable_to_left = X\n",
    "        movable_to_right = X\n",
    "    else:\n",
    "        movable_to_left = [x for x in X if T[0][0] in x]\n",
    "        movable_to_right = [x for x in X if T[-1][1] in x]\n",
    "    return movable_to_left, movable_to_right\n",
    "\n",
    "def check_can_move(X, T):\n",
    "    # Check if the player can move\n",
    "    movable_to_left, movable_to_right = movable_dominos(X, T)\n",
    "    if len(movable_to_left) > 0 or len(movable_to_right) > 0:\n",
    "        can_move = True\n",
    "    else:\n",
    "        can_move = False\n",
    "    return can_move\n",
    "\n",
    "def round_to_5(number):\n",
    "    return round(number / 5) * 5\n",
    "\n",
    "def calculate_points(T):\n",
    "    # Calculate points after adding a domino to the chain\n",
    "    if len(T) == 1 and round_to_5(T[0][0] + T[0][1]) == T[0][0] + T[0][1]:\n",
    "        return T[0][0] + T[0][1]\n",
    "    if len(T) > 1:\n",
    "        left_value = 2*T[0][0] if T[0][0] == T[0][1] else T[0][0]\n",
    "        right_value = 2*T[-1][1] if T[-1][0] == T[-1][1] else T[-1][1]\n",
    "        if round_to_5(left_value + right_value) == left_value + right_value:\n",
    "            return left_value + right_value\n",
    "    return 0\n",
    "\n",
    "def remove_domino(X, domino):\n",
    "    # Remove a domino from a list (e.g. the players hand)\n",
    "    return [x for x in X if not x == domino]\n",
    "\n",
    "def add_domino(T, domino, position):\n",
    "    # Add a domino to a list (e.g. the table)\n",
    "    if len(T) == 0:\n",
    "        T.append(domino)\n",
    "        return T\n",
    "    if position == 'L' and T[0][0] == domino[1]:\n",
    "        T.insert(0, domino)\n",
    "    elif position == 'L' and T[0][0] == domino[0]:\n",
    "        T.insert(0, [domino[1], domino[0]])\n",
    "    elif position == 'R' and T[-1][1] == domino[0]:\n",
    "        T.append(domino)\n",
    "    elif position == 'R' and T[-1][1] == domino[1]:\n",
    "        T.append([domino[1], domino[0]])\n",
    "    return T\n",
    "\n",
    "def move_domino(X, T, domino, position):\n",
    "    # Move a domino from one list to another (e.g. the players hand to the table)\n",
    "    if domino not in X:\n",
    "        raise ValueError(f\"Domino {domino} can't be moved\")\n",
    "    else:\n",
    "        X = remove_domino(X, domino)\n",
    "        T = add_domino(T, domino, position)\n",
    "        points = calculate_points(T)\n",
    "    return X, T, points\n",
    "\n",
    "def draw_dominos(U, X, T):\n",
    "    # Check if the player can move and draw dominoes if necessary\n",
    "    drawn = False\n",
    "    if not check_can_move(X, T):\n",
    "        left_value = T[0][0]\n",
    "        right_value = T[-1][1]\n",
    "        while len(U) > 0 and not drawn:\n",
    "            domino = U.pop(0)\n",
    "            X.append(domino)\n",
    "            if left_value in domino or right_value in domino:\n",
    "                drawn = True\n",
    "        drawn = True\n",
    "    return U, X, drawn\n",
    "\n",
    "def make_move(U, P1, P2, T, points_P1, points_P2, domino = None, position = None):\n",
    "    # A complete make move funtion (calculating points, checking for round and game end)\n",
    "    def total_points(X):\n",
    "        return sum([sum(x) for x in X])\n",
    "    round_winner = None\n",
    "    game_winner = None\n",
    "    points_for_P1 = 0\n",
    "    points_for_P2 = 0\n",
    "\n",
    "    if domino is not None: \n",
    "        P1, T, points_for_P1 = move_domino(P1, T, domino, position)\n",
    "        if len(P1) == 0:\n",
    "            points_for_P1 += round_to_5(total_points(P2))\n",
    "            round_winner = 'P1'\n",
    "    else:\n",
    "        if not check_can_move(P2, T): \n",
    "            points_for_P1 = round_to_5(max(0, total_points(P2) - total_points(P1)))\n",
    "            points_for_P2 = round_to_5(max(0, total_points(P1) - total_points(P2)))       \n",
    "            round_winner = 'P1' if points_for_P1 > points_for_P2 else 'P2'\n",
    "\n",
    "    points_P1 += points_for_P1\n",
    "    points_P2 += points_for_P2\n",
    "\n",
    "    if points_P1 >= 100:\n",
    "        game_winner = 'P1'\n",
    "    elif points_P2 >= 100:\n",
    "        game_winner = 'P2'\n",
    "\n",
    "    return U, P1, T, points_for_P1, points_for_P2, points_P1, points_P2, round_winner, game_winner\n",
    "\n",
    "def choose_domino_at_random(U, X, T):\n",
    "    # Benchmark dummy algorithm - placing a random domino\n",
    "    U, X, drawn = draw_dominos(U, X, T)\n",
    "    grads = 0\n",
    "    movable_to_left, movable_to_right = movable_dominos(X, T)\n",
    "    left_choice = random.choice(movable_to_left) if len(movable_to_left) > 0 else []\n",
    "    right_choice = random.choice(movable_to_right) if len(movable_to_right) > 0 else []\n",
    "    if len(left_choice) > 0 and len(right_choice) > 0:\n",
    "        position = random.choice(['L', 'R'])\n",
    "        if position == 'L':\n",
    "            return U, X, left_choice, 'L', grads, drawn\n",
    "        elif position == 'R':\n",
    "            return U, X, right_choice, 'R', grads, drawn\n",
    "    elif len(left_choice) > 0:\n",
    "        return U, X, left_choice, 'L', grads, drawn\n",
    "    elif len(right_choice) > 0:\n",
    "        return U, X, right_choice, 'R', grads, drawn\n",
    "    else:\n",
    "        return U, X, None, None, grads, drawn\n",
    "    \n",
    "def table_to_observation(amt_O, T, P, history):\n",
    "    # Prepering a model input - function to redefine later\n",
    "    pass\n",
    "\n",
    "def legal_actions(T, P):\n",
    "    # Show legal acctions for the legal actions mask\n",
    "    dominos = [[i, j] for i in range(7) for j in range(i+1)]\n",
    "    legal_actions = np.full(28 * 2, 0)\n",
    "    movable_to_left, movable_to_right = movable_dominos(P, T)\n",
    "    for m in movable_to_left:\n",
    "        legal_actions[dominos.index(m)] = 1\n",
    "    for m in movable_to_right: \n",
    "        legal_actions[dominos.index(m) + 28] = 1\n",
    "    return legal_actions.reshape((1, 56))\n",
    "\n",
    "def translate_action_to_move(action):\n",
    "    # Translate a model output action to a move\n",
    "    if action < 28:\n",
    "        position = 'L'\n",
    "    else:\n",
    "        position = 'R'\n",
    "        action += -28\n",
    "    dominos = [[i, j] for i in range(7) for j in range(i+1)]\n",
    "    domino = dominos[action]\n",
    "    return domino, position\n",
    "    \n",
    "def choose_domino_using_model(len_O, U, P, T, history, model, calculate_grads):\n",
    "    # Choosing a domino using a model\n",
    "    U, P, drawn = draw_dominos(U, P, T)\n",
    "    grads = 0\n",
    "    observation = table_to_observation(len_O, T, P, history)\n",
    "    legal_actions_mask = legal_actions(T, P)\n",
    "\n",
    "    if sum(legal_actions_mask[0]) == 0:\n",
    "        return U, P, None, None, grads, drawn\n",
    "\n",
    "    if calculate_grads:\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model([observation, legal_actions_mask], training=True)\n",
    "            probabilities = predictions[0]\n",
    "            action = np.random.choice(len(probabilities), p=probabilities.numpy())\n",
    "            target = np.zeros(model.output_shape[1]); target[action] = 1\n",
    "            y_target = tf.convert_to_tensor(target, dtype=tf.float32)\n",
    "            loss = tf.reduce_mean(model.loss(y_target, probabilities))\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables) # Computing gradients\n",
    "\n",
    "    else:\n",
    "\n",
    "        predictions = model([observation, legal_actions_mask], training=True)\n",
    "        probabilities = predictions[0]\n",
    "        action = np.random.choice(len(probabilities), p=probabilities.numpy())\n",
    "\n",
    "    domino, position = translate_action_to_move(action)\n",
    "\n",
    "    if domino not in P:\n",
    "        print(predictions[0])\n",
    "        raise ValueError(f\"Model chose illegal action: moving {domino} {position}\")\n",
    "\n",
    "    return U, P, domino, position, grads, drawn\n",
    "\n",
    "def calc_rewards(whose_move, O_dominos, T_dominos, P_dominos, points_earned, had_to_draw, round_winners, game_winners, \n",
    "                 all_grads):\n",
    "    # Defining rewards for model calibration - function to redefine later\n",
    "    P_grads, O_grads, P_rewards, O_rewards = None, None, None, None\n",
    "    return P_grads, O_grads, P_rewards, O_rewards\n",
    "\n",
    "def play_one_game(P_model, O_model, print_rounds = False, calculate_grads = True):\n",
    "    # Play a complete game of dominos between two players (random vs. random, random vs. model, model vs. model)\n",
    "    starting_player, round_winner, game_winner = random.choice(['P', 'O']), None, None\n",
    "    P_points, O_points, i = 0, 0, 0\n",
    "    points_earned, round_winners, game_winners, whose_move, had_to_draw, P_dominos, O_dominos, T_dominos, all_grads = \\\n",
    "        [], [], [], [], [], [], [], [], []\n",
    "\n",
    "    while game_winner is None and i < 1000:\n",
    "        round_winner = None\n",
    "        U, P, O, T = start_round()\n",
    "        print_board(U, O, T, P, P_points, O_points) if print_rounds else None\n",
    "        \n",
    "        while round_winner is None and game_winner is None and i < 1000:\n",
    "            last_moved=None\n",
    "            if starting_player == 'P':\n",
    "                i += 1\n",
    "                if P_model == 'random':\n",
    "                    U, P, domino, position, grads_P, drew_domino = choose_domino_at_random(U, P, T)\n",
    "                else:\n",
    "                    U, P, domino, position, grads_P, drew_domino = \\\n",
    "                        choose_domino_using_model(O, U, P, T, (round_winners, whose_move, had_to_draw, T_dominos, 'P'), \n",
    "                                                  P_model, calculate_grads)\n",
    "                U, P, T, points_for_P, points_for_O, P_points, O_points, round_winner, game_winner = \\\n",
    "                    make_move(U, P, O, T, P_points, O_points, domino, position)\n",
    "                last_moved = 'P'\n",
    "                print(f'Move by P, move {i}') if print_rounds else None\n",
    "                print_board(U, O, T, P, P_points, O_points) if print_rounds else None\n",
    "                if round_winner is not None:\n",
    "                    round_winner = 'P' if round_winner == 'P1' else 'O'\n",
    "                    starting_player = round_winner\n",
    "                if game_winner is not None:\n",
    "                    game_winner = 'P' if game_winner == 'P1' else 'O'\n",
    "\n",
    "                whose_move.append(last_moved)\n",
    "                O_dominos.append(O.copy())\n",
    "                T_dominos.append(T.copy())\n",
    "                P_dominos.append(P.copy())\n",
    "                points_earned.append(points_for_P - points_for_O)\n",
    "                had_to_draw.append(drew_domino)\n",
    "                round_winners.append(round_winner)\n",
    "                game_winners.append(game_winner)\n",
    "                all_grads.append(grads_P)\n",
    "\n",
    "            if round_winner is None and game_winner is None:\n",
    "                starting_player = 'P'\n",
    "                i += 1\n",
    "                if O_model == 'random':\n",
    "                    U, O, domino, position, grads_O, drew_domino = choose_domino_at_random(U, O, T)\n",
    "                else:\n",
    "                    U, O, domino, position, grads_O, drew_domino = \\\n",
    "                        choose_domino_using_model(P, U, O, T, (round_winners, whose_move, had_to_draw, T_dominos, 'O'), \n",
    "                                                  O_model, calculate_grads)\n",
    "                U, O, T, points_for_O, points_for_P, O_points, P_points, round_winner, game_winner = \\\n",
    "                    make_move(U, O, P, T, O_points, P_points, domino, position)\n",
    "                last_moved = 'O'\n",
    "                print(f'Move by O, move {i}') if print_rounds else None\n",
    "                print_board(U, O, T, P, P_points, O_points) if print_rounds else None\n",
    "                if round_winner is not None:\n",
    "                    round_winner = 'O' if round_winner == 'P1' else 'P'\n",
    "                    starting_player = round_winner\n",
    "                if game_winner is not None:\n",
    "                    game_winner = 'O' if game_winner == 'P1' else 'P'\n",
    "\n",
    "                whose_move.append(last_moved)\n",
    "                O_dominos.append(O.copy())\n",
    "                T_dominos.append(T.copy())\n",
    "                P_dominos.append(P.copy())\n",
    "                points_earned.append(points_for_O - points_for_P)\n",
    "                had_to_draw.append(drew_domino)\n",
    "                round_winners.append(round_winner)\n",
    "                game_winners.append(game_winner)\n",
    "                all_grads.append(grads_O)\n",
    "\n",
    "        print(f'Round winner: {round_winner}') if print_rounds else None\n",
    "        print(f'Game winner: {game_winner}') if print_rounds and game_winner is not None else None\n",
    "\n",
    "        P_grads, O_grads, P_rewards, O_rewards = calc_rewards(whose_move, O_dominos, T_dominos, \n",
    "                                                              P_dominos, points_earned, had_to_draw, \n",
    "                                                              round_winners, game_winners, all_grads)\n",
    "    # return whose_move, O_dominos, T_dominos, P_dominos, points_earned, had_to_draw, round_winners, game_winners, all_grads # To use to design new reward functions\n",
    "    return game_winner, P_grads, O_grads, P_rewards, O_rewards, \n",
    "\n",
    "def check_model_vs_model_distribution(P_model, O_model, stats_amount, games_amt, \n",
    "                                      print_rounds = False, calculate_grads = False):\n",
    "    # A function to visually compare the performance of two models\n",
    "    P_win_percentage = []\n",
    "\n",
    "    for i in range(stats_amount):\n",
    "        all_games = []\n",
    "        for j in range(games_amt):\n",
    "            game_winner, P_grads, O_grads, P_rewards, O_rewards = play_one_game(P_model, O_model, print_rounds, calculate_grads)\n",
    "            all_games.append(game_winner)\n",
    "\n",
    "        counts = Counter(all_games)\n",
    "        percent_O = counts['O'] / games_amt\n",
    "        percent_P = counts['P'] / games_amt\n",
    "\n",
    "        P_win_percentage.append(percent_P)\n",
    "\n",
    "    print(f'P_win_percentage mean: {round(100*np.mean(P_win_percentage), 1)}%, std: {round(100*np.std(P_win_percentage), 1)}%')\n",
    "\n",
    "    sns.histplot(P_win_percentage, kde=True, bins=10, color='blue').set_title('Distribution of P_win_percentage')\n",
    "\n",
    "# Functions for model traininng\n",
    "\n",
    "def play_sound():\n",
    "    # Inform about calibration ending\n",
    "    sa.play_buffer((0.5 * np.sin(2 * np.pi * 440 * np.linspace(0, 1, 44100)) * (2**15 - 1)\n",
    "                    ).astype(np.int16), 1, 2, 44100).wait_done()\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    # Discounting rewards from a move over the whole game\n",
    "    rewards = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        rewards[step] += rewards[step + 1] * discount_factor\n",
    "    return rewards\n",
    "\n",
    "def discount_rewards_per_round(rewards, round_winners, game_winners, discount_factor):\n",
    "    # Discount rewards from a move over the whole round\n",
    "    rewards = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        if round_winners[step] is None and game_winners[step] is None:\n",
    "            rewards[step] += rewards[step + 1] * discount_factor\n",
    "    return rewards\n",
    "\n",
    "def normalize_rewards(rewards):\n",
    "    # Normalize the rewards by centering them and setting std to 1\n",
    "    rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
    "    return rewards   \n",
    "\n",
    "def weigh_grads(gradients, rewards):\n",
    "    # Weigh gradients with rewrards\n",
    "    weighted_grads = []\n",
    "    for i in range(len(gradients)):\n",
    "        if gradients[i] != 0:\n",
    "            discounted_grads = [grad * rewards[i] for grad in gradients[i]]\n",
    "            weighted_grads.append(discounted_grads)\n",
    "    has_nans = any(tf.reduce_any(tf.math.is_nan(grad)) for grads in weighted_grads for grad in grads)\n",
    "    if has_nans:\n",
    "        return None\n",
    "    else:\n",
    "        return weighted_grads\n",
    "    \n",
    "def calc_mean_grads(grads):\n",
    "    # Add gradients from multiple steps into one gradient for computational efficiency\n",
    "    transposed_grads = list(zip(*grads))\n",
    "    mean_grads = []\n",
    "    for transposed_grad in transposed_grads:\n",
    "        mean_grads.append(tf.reduce_mean(transposed_grad, axis=0))\n",
    "    return mean_grads\n",
    "\n",
    "def update_model(optimizer, model, gradients, rewards):\n",
    "    # Update the model using the weighted gradients\n",
    "    weighted_grads = weigh_grads(gradients, rewards)\n",
    "\n",
    "    if weighted_grads is not None:\n",
    "        mean_grads = calc_mean_grads(weighted_grads)\n",
    "        optimizer.apply_gradients(zip(mean_grads, model.trainable_variables))\n",
    "    else:\n",
    "        print(\"NaN values detected in weighted gradients. Model parameters were not updated.\")\n",
    "    return model\n",
    "\n",
    "def plot_rewards(rewards_for_model, lag = 10, lag2 = 100):\n",
    "    # Plot rewards to track the training process\n",
    "    mean_rewards = [np.mean(rewards_for_model[i-lag:i]) for i in range(lag, len(rewards_for_model) + 1)]\n",
    "    mean_rewards2 = [np.mean(rewards_for_model[i-lag2:i]) for i in range(lag2, len(rewards_for_model) + 1)]\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.plot(range(lag, len(rewards_for_model) + 1), mean_rewards, 'orange')\n",
    "    plt.plot(range(lag2, len(rewards_for_model) + 1), mean_rewards2, 'black')\n",
    "    plt.axhline(y=mean_rewards2[-1], color='lightgray', linestyle='--') if mean_rewards2 != [] else None\n",
    "    plt.xlabel('Episode')\n",
    "    plt.title(f'Mean rewards for the model over {lag} (orange) and {lag2} (black) episodes')\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, calibrate_from_model, benchmark_for_stopping_calibration = 80, text = '', benchmark_for_halving_lr = 0, \n",
    "                learning_rate=0.0025, limit_of_simulations = 10000, amt_games_to_average = 4):\n",
    "    # Train the model by playing against the random player or another model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    rewards_for_model = []\n",
    "    all_games_winner = []\n",
    "    learning_rate_halved = False\n",
    "\n",
    "    if calibrate_from_model:\n",
    "        benchmark_model = keras.models.clone_model(model)\n",
    "        benchmark_model.set_weights(model.get_weights())\n",
    "\n",
    "    for i in range(round(limit_of_simulations/amt_games_to_average)):\n",
    "\n",
    "        all_P_rewards = []\n",
    "        all_P_grads = []\n",
    "\n",
    "        for j in range(amt_games_to_average): # We update model only after averaging over amt_games_to_average (e.g. 4) games\n",
    "\n",
    "            if calibrate_from_model:\n",
    "                game_winner, P_grads, O_grads, P_rewards, O_rewards = play_one_game(model, benchmark_model, \n",
    "                                                                                    print_rounds = False, calculate_grads = True)\n",
    "            else:\n",
    "                game_winner, P_grads, O_grads, P_rewards, O_rewards = play_one_game(model, 'random', \n",
    "                                                                                    print_rounds = False, calculate_grads = True)\n",
    "            \n",
    "            sum_P_rewards = sum(P_rewards)\n",
    "\n",
    "            rewards_for_model.append(sum_P_rewards)\n",
    "\n",
    "            all_games_winner.append(game_winner)\n",
    "\n",
    "            if sum_P_rewards != 0:\n",
    "                all_P_rewards = all_P_rewards + P_rewards\n",
    "                all_P_grads = all_P_grads + P_grads\n",
    "\n",
    "        all_P_rewards_normalized = normalize_rewards(all_P_rewards)\n",
    "        model = update_model(optimizer, model, all_P_grads, all_P_rewards_normalized)\n",
    "\n",
    "        games_won_by_model = sum([1 if w == 'P' else 0 for w in all_games_winner[-101:-1]])\n",
    "\n",
    "        if (i+1) % 5 == 0: # Check the gradient impact\n",
    "            clear_output(wait=True)\n",
    "            avg_reward = np.mean(rewards_for_model[-101:-1])\n",
    "            print(text)\n",
    "            print(f'Step {amt_games_to_average*(i + 1)}')\n",
    "            print(f'Rewards in last game: {P_rewards}')\n",
    "            print(f'! Percentage of games won by model in last 100 games: {games_won_by_model}%')\n",
    "            if benchmark_for_halving_lr != 0 and not learning_rate_halved and games_won_by_model > benchmark_for_halving_lr:\n",
    "                learning_rate = learning_rate / 2\n",
    "                optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "                learning_rate_halved = True\n",
    "            print(f'! Average reward in last 100 games: {np.round(avg_reward,2)}')\n",
    "            print(f'Learning rate in last game: {learning_rate}')\n",
    "            for j in range(len(model.trainable_variables)):\n",
    "                if P_grads[-1] != 0:\n",
    "                    update_ratio = np.abs(P_grads[-1][j].numpy()).sum() / np.abs(model.trainable_variables[j].numpy()).sum()\n",
    "                    print(f\"Sum of gradients: {np.abs(P_grads[-1][j].numpy()).sum():.5f}, fraction of model variables: {update_ratio:.5f}\")\n",
    "            plot_rewards(rewards_for_model, lag = 10, lag2 = 100)\n",
    "\n",
    "        if games_won_by_model > benchmark_for_stopping_calibration:\n",
    "            break # End training if the model wins enough of the last 100 games\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the reinforcement learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_observation(len_O, T, P, history):\n",
    "    # Translating the game into an observation for the model\n",
    "    def find_index_of_full_domino(domino):\n",
    "        dominos = [[i, j] for i in range(7) for j in range(i+1)]\n",
    "        sorted_domino = sorted(domino, reverse=True)\n",
    "        index = dominos.index(sorted_domino)\n",
    "        return index\n",
    "    def find_index_of_half_domino(domino, position):\n",
    "        if domino[0] == domino[1]:\n",
    "            index = 7 + domino[0]\n",
    "        else:\n",
    "            index = domino[0] if position == 'L' else domino[1]\n",
    "        return index\n",
    "    def count_domino_halfs(X):\n",
    "        placed_half_dominos = np.zeros(7)\n",
    "        for domino in X:\n",
    "            placed_half_dominos[domino[0]] += 1\n",
    "            placed_half_dominos[domino[1]] += 1 if domino[0] != domino[1] else 0\n",
    "        return placed_half_dominos\n",
    "    def calculate_values_that_drew(history):\n",
    "        round_winners, whose_move, had_to_draw, T_dominos, now_to_move = history # The history of the game\n",
    "        drawn_values = np.zeros(7)\n",
    "        for i in range(len(whose_move) - 1):\n",
    "            i += 1\n",
    "            if round_winners[-i] is not None:\n",
    "                break\n",
    "            if whose_move[-i] != now_to_move and had_to_draw[-i]:\n",
    "                drawn_values[T_dominos[-i-1][0][0]] = 1\n",
    "                drawn_values[T_dominos[-i-1][-1][1]] = 1\n",
    "                break\n",
    "        return drawn_values\n",
    "              \n",
    "    table = np.full(28*3, 0)\n",
    "    \n",
    "    if len(T) > 0:\n",
    "        table[find_index_of_half_domino(T[0], 'L')] = 1\n",
    "        table[14 + find_index_of_half_domino(T[-1], 'R')] = 1\n",
    "\n",
    "    for p in P:\n",
    "        table[28 + find_index_of_full_domino(p)] = 1\n",
    "    for t in T:\n",
    "        table[28*2 + find_index_of_full_domino(t)] = 1\n",
    "\n",
    "    table = np.concatenate((table, calculate_values_that_drew(history)))\n",
    "\n",
    "    return table.reshape(1, len(table))\n",
    "\n",
    "def calc_rewards(whose_move, O_dominos, T_dominos, P_dominos, points_earned, had_to_draw, round_winners, game_winners, all_grads):\n",
    "    # Defining the final reward structure that will be used\n",
    "    P_rewards, P_grads = [], []\n",
    "    O_rewards, O_grads = [], []\n",
    "    P_rewards_for_winning_rounds = list(np.zeros(len(whose_move)))\n",
    "    P_rewards_for_drawing = list(np.zeros(len(whose_move)))\n",
    "    P_rewards_for_being_protected = list(np.zeros(len(whose_move)))\n",
    "    P_rewards_for_earning_points = list(np.zeros(len(whose_move)))\n",
    "    P_rewards_for_diversification = list(np.zeros(len(whose_move)))\n",
    "    P_rewards_for_placing_double = list(np.zeros(len(whose_move)))\n",
    "\n",
    "    def number_in_dominos(element, list_of_lists):\n",
    "        return any(element in sublist for sublist in list_of_lists)\n",
    "    \n",
    "    def filter_lists(list_to_filter, list_of_criteria, searched_criteria):\n",
    "        return [list_to_filter[i] for i in range(len(list_of_criteria)) if list_of_criteria[i] == searched_criteria]\n",
    "    \n",
    "    def sum_lists(lists):\n",
    "        return [round(sum(elements), 2) for elements in zip(*lists)]\n",
    "    \n",
    "    def placed_a_double_domino(T, T_before):\n",
    "        placed_double = False\n",
    "        if T[0] != T_before[0] and T[0][0] == T[0][1]:\n",
    "            placed_double = True\n",
    "        elif T[-1] != T_before[-1] and T[-1][0] == T[-1][1]:\n",
    "            placed_double = True\n",
    "        return placed_double\n",
    "\n",
    "    P_grads = filter_lists(all_grads, whose_move, 'P')\n",
    "\n",
    "    points_in_round_P = 0\n",
    "    points_in_round_O = 0\n",
    "    for i in range(len(whose_move)):\n",
    "        # Rewards for winning a round\n",
    "        if whose_move[i] == 'P':\n",
    "            points_in_round_P += points_earned[i]\n",
    "        elif whose_move[i] == 'O':\n",
    "            points_in_round_O += points_earned[i]\n",
    "        if round_winners[i] is not None or game_winners[i] is not None:\n",
    "            if points_in_round_P >= points_in_round_O:\n",
    "                P_rewards_for_winning_rounds[i] += 2.5\n",
    "            else:\n",
    "                P_rewards_for_winning_rounds[i] += -2.5\n",
    "            points_in_round_P, points_in_round_O = 0, 0\n",
    "        # Rewards for making opponent draw\n",
    "        if round_winners[i] is None and game_winners[i] is None and \\\n",
    "            whose_move[i] == 'P' and whose_move[i+1] == 'O' and had_to_draw[i+1]:\n",
    "                P_rewards_for_drawing[i] += 10 # 7.5\n",
    "        # Rewards for being protected from both sides\n",
    "        if len(T_dominos[i]) > 0 and len(P_dominos[i]) > 0 and number_in_dominos(T_dominos[i][0][0], P_dominos) and \\\n",
    "            number_in_dominos(T_dominos[i][-1][1], P_dominos):\n",
    "                    P_rewards_for_being_protected[i] += 2.5\n",
    "        # Reward for getting rid of a double domino\n",
    "        if i > 0 and round_winners[i] is None and game_winners[i] is None and \\\n",
    "            whose_move[i] == 'P' and whose_move[i-1] == 'O' and placed_a_double_domino(T_dominos[i], T_dominos[i-1]):\n",
    "            P_rewards_for_placing_double[i] = 2.5\n",
    "        # Reward for having a diversified set of dominos\n",
    "        if whose_move[i] == 'P':\n",
    "             for x in range(0):\n",
    "                  P_rewards_for_diversification[i] += number_in_dominos(x, P_dominos) / 2\n",
    "        # Rewards for earning points\n",
    "        P_rewards_for_earning_points[i] = points_earned[i]\n",
    "        # Punishment for the opponent earning points\n",
    "        if round_winners[i] is None and game_winners[i] is None and \\\n",
    "            whose_move[i] == 'P' and whose_move[i+1] == 'O':\n",
    "            P_rewards_for_earning_points[i] += min(points_earned[i+1], 20) / 5\n",
    "\n",
    "    P_rewards_for_winning_rounds = discount_rewards_per_round(P_rewards_for_winning_rounds, round_winners, game_winners, 0.99)\n",
    "\n",
    "    P_rewards_for_winning_rounds = filter_lists(P_rewards_for_winning_rounds, whose_move, 'P')\n",
    "    P_rewards_for_drawing = filter_lists(P_rewards_for_drawing, whose_move, 'P')\n",
    "    P_rewards_for_being_protected = filter_lists(P_rewards_for_being_protected, whose_move, 'P')\n",
    "    P_rewards_for_earning_points = filter_lists(P_rewards_for_earning_points, whose_move, 'P')\n",
    "    P_rewards_for_diversification = filter_lists(P_rewards_for_diversification, whose_move, 'P')\n",
    "    P_rewards_for_placing_double = filter_lists(P_rewards_for_placing_double, whose_move, 'P')\n",
    "\n",
    "    P_rewards = sum_lists([P_rewards_for_winning_rounds,\n",
    "                           P_rewards_for_drawing,\n",
    "                           P_rewards_for_being_protected,\n",
    "                           P_rewards_for_earning_points,\n",
    "                           P_rewards_for_diversification,\n",
    "                           P_rewards_for_placing_double])\n",
    "\n",
    "    return P_grads, O_grads, P_rewards, O_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable() # Registring the custom layer with Keras\n",
    "class MaskedSoftmax(keras.layers.Layer):\n",
    "    def call(self, inputs, mask):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        masked_logits = inputs + (1 - mask) * -1e9  # Setting probability of illegal actions to 0\n",
    "        return tf.nn.softmax(masked_logits)\n",
    "\n",
    "n_inputs = 28 * 3 + 7\n",
    "n_outputs = 56\n",
    "dropout_rate = 0.1\n",
    "\n",
    "observation_input = keras.layers.Input(shape=(n_inputs,))\n",
    "legal_actions_input = keras.layers.Input(shape=(n_outputs,))\n",
    "x = keras.layers.Dense(100, activation=\"elu\")(observation_input)\n",
    "x = keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "x = keras.layers.Dense(100, activation=\"elu\")(x)\n",
    "x = keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "logits = keras.layers.Dense(n_outputs)(x)\n",
    "action_probabilities = MaskedSoftmax()(logits, legal_actions_input)\n",
    "\n",
    "loss_fn = keras.losses.categorical_crossentropy\n",
    "\n",
    "model = keras.models.Model(inputs=[observation_input, legal_actions_input], outputs=action_probabilities)\n",
    "model.compile(loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that pre-training model vs. random algorithm wins 50% of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_model_vs_model_distribution(model, 'random', 64, 64, print_rounds = False, calculate_grads = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = 'dominoes.h5'\n",
    "\n",
    "# # UNCOMMENT IF YOU WANT TO TRAIN THE NETWORK\n",
    "\n",
    "# times = []\n",
    "\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = False, benchmark_for_stopping_calibration = 75, text = 'TRAINING STEP 1')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = True, benchmark_for_stopping_calibration = 65, text = 'TRAINING STEP 2')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = False, benchmark_for_stopping_calibration = 80, text = 'TRAINING STEP 3')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = True, benchmark_for_stopping_calibration = 65, text = 'TRAINING STEP 4')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = False, benchmark_for_stopping_calibration = 85, text = 'TRAINING STEP 5')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = True, benchmark_for_stopping_calibration = 60, text = 'TRAINING STEP 6')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = False, benchmark_for_stopping_calibration = 90, text = 'TRAINING STEP 7')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = True, benchmark_for_stopping_calibration = 60, text = 'TRAINING STEP 8')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "# model = train_model(model, calibrate_from_model = True, benchmark_for_stopping_calibration = 60, text = 'TRAINING STEP 9')\n",
    "# times.append(dt.datetime.now().strftime(\"%Y-%m-%d %H%:%M:%S\"))\n",
    "\n",
    "# print(f'times: {times}')\n",
    "\n",
    "# play_sound()\n",
    "\n",
    "# # model.save(model_file_path)\n",
    "\n",
    "# IF YOU WANT TO LOAD A TRAINED NEURAL NETWORK\n",
    "\n",
    "model = keras.models.load_model(model_file_path, custom_objects={'MaskedSoftmax': MaskedSoftmax})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model performance after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model_vs_model_distribution(model, 'random', 100, 100, print_rounds = False, calculate_grads = False) # 82.3%, std: 3.7%\n",
    "play_sound()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at a game played by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = play_one_game(model, 'random', print_rounds = True, calculate_grads = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
